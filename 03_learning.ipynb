{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adjusting the weights to learn\n",
    "\n",
    "<p>This notebook shows how to train the network by adjusting the weights\n",
    "\n",
    "<small>Author: Fernando Carlos López Hernández</small>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def init_random_NN(Ns_per_layer):\n",
    "    \"\"\" Create a NN with the number of Ns indicated for each input+hidden+output layer \"\"\"\n",
    "    NN = list()\n",
    "    n_previous = Ns_per_layer.pop(0)\n",
    "    for n in Ns_per_layer:\n",
    "        layer = [{'w': np.random.normal(0,1, size = n_previous+1)} for i in range(n)]\n",
    "        NN.append(layer)\n",
    "        n_previous = n\n",
    "    return NN\n",
    "\n",
    "def activation_value(weights, inputs):\n",
    "    \"\"\" Computes the activation value of the N \"\"\"\n",
    "    return np.dot(weights, inputs)\n",
    "\n",
    "def sigmoid(v):\n",
    "    return 1.0 / (1.0 + np.exp(-v))\n",
    "\n",
    "def step(v):\n",
    "    if v>0.0:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def forward_propagate(NN, inputs, transfer_fn):\n",
    "    inputs = np.concatenate([[1.0],inputs])\n",
    "    for L in NN:\n",
    "        outputs = []\n",
    "        for N in L:\n",
    "            N['v'] = activation_value(N['w'], inputs)\n",
    "            N['y'] = transfer_fn(N['v'])\n",
    "            outputs.append(N['y'])\n",
    "        inputs = np.concatenate([[1.0],outputs])\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the NN problem\n",
    "First we create a NN with random weights and the structure of the AND, OR, XOR problem to solve.  2 inputs and 3 outputs (AND, OR, XOR)\n",
    "\n",
    "X contains the samples by columns\n",
    "\n",
    "G contains the ground truth of each sample by columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We define the AND, OR, XOR problem using one hot encoding\n",
    "X = np.array([[1, 1, 1, 1],\n",
    "              [0, 0, 1, 1],\n",
    "              [0, 1, 0, 1]])\n",
    "G = np.array([[0, 0, 0, 1],\n",
    "              [0, 1, 1, 1],\n",
    "              [0, 1, 1, 0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training with a single layer\n",
    "\n",
    "Then we initializate the NN with random weigths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[{'w': array([0.03649985, 1.28319384, 0.28645509])}, {'w': array([-0.70454226, -0.07538271, -0.60932087])}, {'w': array([ 2.02422437, -0.51085988, -0.63239757])}]]\n"
     ]
    }
   ],
   "source": [
    "n_inputs = X.shape[0]-1\n",
    "n_outputs = G.shape[0]\n",
    "NN = init_random_NN([n_inputs,n_outputs])\n",
    "print(NN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we train the single layer NN using the delta rule."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_single_layer(NN, X, G, transfer_fn, max_epochs = 1000, alpha = 0.1, loss_threshold = 0.01):\n",
    "    \"\"\" Update the weights until the loss is less than loss_threshold\n",
    "        X has the training set batch with the samples in columns\n",
    "        G has the expected output values for each sample in a columns \"\"\"\n",
    "    for _ in range(max_epochs):\n",
    "        epoch_loss = 0.0\n",
    "        for s in range(X.shape[1]):\n",
    "            # For each sample\n",
    "            input_sample = X[:,s]\n",
    "            ground_sample = G[:,s]\n",
    "            # Forward-propagate providing the input_sample without bias\n",
    "            y_pred = np.array(forward_propagate(NN, input_sample[1:], transfer_fn))\n",
    "            # For each N in the output layer\n",
    "            output_layer = NN[-1]\n",
    "            for i,N in enumerate(output_layer):\n",
    "                # Error estimation for N with the sample\n",
    "                e = ground_sample[i] - y_pred[i]\n",
    "                epoch_loss += e**2\n",
    "                # Delta Learning rule\n",
    "                delta = y_pred[i]*(1-y_pred[i])*e\n",
    "                dW = alpha*delta*input_sample\n",
    "                # Update the weights\n",
    "                N['w'] += dW\n",
    "        if (epoch_loss<=loss_threshold):\n",
    "            break\n",
    "\n",
    "transfer_fn = sigmoid\n",
    "max_epochs = 10000\n",
    "train_single_layer(NN, X, G, transfer_fn, max_epochs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We show the results with the trained NN. Note that the NN trained with the delta rule correctly predicts the AND and OR gates, but the NN fails recognizing the XOR gate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INPUT  [0 0]  AND y:0.000243: OR y:0.055037: NOR y:0.503144\n",
      "INPUT  [0 1]  AND y:0.055452: OR y:0.965638: NOR y:0.500000\n",
      "INPUT  [1 0]  AND y:0.055471: OR y:0.965633: NOR y:0.496856\n",
      "INPUT  [1 1]  AND y:0.934074: OR y:0.999926: NOR y:0.493711\n"
     ]
    }
   ],
   "source": [
    "inputs = X[1:,:]\n",
    "for input in inputs.T:\n",
    "    y_pred = forward_propagate(NN, input, transfer_fn)\n",
    "    print('INPUT ', input, ' AND y:%f: OR y:%f: NOR y:%f' %  tuple(y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training with backpropagation\n",
    "\n",
    "First we initialize the NN with a hidden layer and random weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[{'w': array([0.28468051, 0.12332044, 1.9822295 ])}, {'w': array([-0.66979376, -0.02946977,  1.06339887])}, {'w': array([-0.52217385, -0.05858228,  0.93349379])}], [{'w': array([ 0.1924408 , -0.69651302, -0.05787673, -0.1032432 ])}, {'w': array([-0.26851213,  0.34483267, -0.68647113, -1.02890057])}, {'w': array([ 0.52742456, -0.18107997,  0.72957427,  0.29135883])}]]\n"
     ]
    }
   ],
   "source": [
    "n_hiddens = max(n_inputs, n_outputs)\n",
    "NN = init_random_NN([n_inputs,n_hiddens,n_outputs])\n",
    "print(NN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we train the NN using backpropagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def N_transfer_derivative(y_pred):\n",
    "    return y_pred * (1.0 - y_pred)\n",
    "\n",
    "def backward_propagate_error(NN, ground_sample):\n",
    "    \"\"\" Backward propagate the error in the NN as currently configured by forward_propagate \"\"\"\n",
    "    for l in reversed(range(len(NN))):\n",
    "        L = NN[l]\n",
    "        errors = []\n",
    "        if l == len(NN)-1: # Output layer\n",
    "            for i in range(len(L)):\n",
    "                N = L[i]\n",
    "                N['e'] = ground_sample[i] - N['y']\n",
    "                errors.append(N['e'])\n",
    "        else: # Hidden layer\n",
    "            for i in range(1,len(L)+1): # Skip the next layer bias\n",
    "                N = L[i-1]\n",
    "                N['e'] = 0.0\n",
    "                for forwardN in NN[l+1]:\n",
    "                    N['e'] += forwardN['w'][i] * forwardN['d']\n",
    "                errors.append(N['e'])\n",
    "        # Compute deltas and updates: is the same in all the layers\n",
    "        for i in range(len(L)):\n",
    "            N = L[i]\n",
    "            N['d'] = errors[i] * N_transfer_derivative(N['y'])\n",
    "\n",
    "def update_weights(NN, inputs, alpha):\n",
    "    \"\"\" Update weights forward from the input sample \"\"\"\n",
    "    for l in range(len(NN)):\n",
    "        for N in NN[l]:\n",
    "            N['w'][0] += alpha*N['d']*1.0\n",
    "            for i in range(len(inputs)):\n",
    "                N['w'][i+1] += alpha*N['d']*inputs[i]\n",
    "        inputs = [N['y'] for N in NN[l]]\n",
    "\n",
    "def train_backpropagation(NN, X, G, transfer_fn, max_epochs = 1000, alpha = 0.1, loss_threshold = 0.01):\n",
    "    \"\"\" Update the weights until the loss is less than loss_threshold\n",
    "        X has the training set batch with the samples in columns\n",
    "        G has the expected output values for each sample in a columns \"\"\"\n",
    "    X = X[1:,:] # Remove the biases as they are always 1.0\n",
    "    n_samples = X.shape[1]\n",
    "    n_outputs = G.shape[0]\n",
    "    for epoch in range(max_epochs):\n",
    "        epoch_loss = 0.0\n",
    "        for s in range(n_samples):\n",
    "            # For each sample\n",
    "            input_sample = X[:,s]\n",
    "            ground_sample = G[:,s]\n",
    "            y_pred = np.array(forward_propagate(NN, input_sample, transfer_fn))\n",
    "            epoch_loss += sum( (ground_sample[i]-y_pred[i])**2 for i in range(n_outputs))\n",
    "            backward_propagate_error(NN, ground_sample)\n",
    "            update_weights(NN, input_sample, alpha)\n",
    "        if (epoch_loss<=loss_threshold):\n",
    "            break\n",
    "\n",
    "# We train the multi layer NN using backpropagation\n",
    "transfer_fn = sigmoid \n",
    "max_epochs = 1000\n",
    "alpha = 0.5\n",
    "train_backpropagation(NN, X, G, transfer_fn, max_epochs, alpha)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We show the results with the trained NN. Note that now the NN can recognize all the gates, including the XOR gate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INPUT  [0 0]  AND y:0.000452: OR y:0.053638: XOR y:0.123319\n",
      "INPUT  [0 1]  AND y:0.040539: OR y:0.967055: XOR y:0.927659\n",
      "INPUT  [1 0]  AND y:0.044682: OR y:0.961518: XOR y:0.861161\n",
      "INPUT  [1 1]  AND y:0.934601: OR y:0.997399: XOR y:0.130921\n"
     ]
    }
   ],
   "source": [
    "inputs = X[1:,:]\n",
    "for input in inputs.T:\n",
    "    y_pred = forward_propagate(NN, input, transfer_fn)\n",
    "    print('INPUT ', input, ' AND y:%f: OR y:%f: XOR y:%f' %  tuple(y_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d2152fd7f0bbc62aa1baff8c990435d1e2c7175d001561303988032604c11a48"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
